{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344f364a0ecf68e0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T21:15:37.919976Z",
     "start_time": "2024-05-07T21:15:31.765536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 22:15:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from src.data_generator.csv_data_processor import CSVDataProcessor\n",
    "from src.utils.statistcs.statistical_functions import create_distributed_age_df\n",
    "from spark_instance import spark\n",
    "from src.utils.statistcs.data_visualisations import plot_age_distribution_with_sd, plot_kernel_density_age_distribution\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_age = 40.2\n",
    "\n",
    "csv_age_file_file = CSVDataProcessor(spark, \"data/uk/uk_age_population.csv\")\n",
    "\n",
    "csv_age_uk_df = csv_age_file_file.runner()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48c373a8586c425e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running outside Docker.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "docker_indicators = [\"/.dockerenv\", \"/proc/self/cgroup\"]\n",
    "docker_found = any(os.path.exists(indicator) for indicator in docker_indicators)\n",
    "if docker_found:\n",
    "    print(\"Running inside a Docker container.\")\n",
    "else:\n",
    "    print(\"Running outside Docker.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T20:32:24.127621Z",
     "start_time": "2024-05-07T20:32:24.097891Z"
    }
   },
   "id": "7f29b371513bbf74"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "spark.sql(\"USE healthcare\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T20:32:27.195204Z",
     "start_time": "2024-05-07T20:32:27.164734Z"
    }
   },
   "id": "455f7a02c0c3c585"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from data_generator.base_data_creator import BaseDataCreator\n",
    "\n",
    "bdc = BaseDataCreator(spark, \"uk\")\n",
    "base_df = bdc.runner()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T21:15:55.139331Z",
     "start_time": "2024-05-07T21:15:42.784538Z"
    }
   },
   "id": "545ae6122cacd85"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+------+----------------+--------------------+--------+---------+------------+------------+--------------------+\n",
      "|Age|       DOB|Blood_type|Gender|            Name|           Ethnicity|Postcode|is_female|is_pediatric|is_geriatric|           unique_id|\n",
      "+---+----------+----------+------+----------------+--------------------+--------+---------+------------+------------+--------------------+\n",
      "|  0|2024-04-25|        O-|  Male|      J. Edwards|         White other| EX6 3YJ|    false|        true|       false|J_Edwards|2024-04...|\n",
      "|  0|2023-07-04|        A+|Female|    F. O. Taylor|       White English| OX4 3SY|     true|        true|       false|F_O_Taylor|2023-0...|\n",
      "|  0|2023-09-25|        O-|  Male|     Z. McDonald|       White English| NE7 3XP|    false|        true|       false|Z_McDonald|2023-0...|\n",
      "|  0|2023-12-02|        B+|  Male|        Y. Lyons|Mixed White/Black...| BL7 0WN|    false|        true|       false|Y_Lyons|2023-12-0...|\n",
      "|  0|2023-08-09|        B+|Female|  F. W. Williams|       White English| DD6 3XO|     true|        true|       false|F_W_Williams|2023...|\n",
      "|  0|2024-03-11|        O+|  Male|    D. G. Barlow|       White English| MK5 9XG|    false|        true|       false|D_G_Barlow|2024-0...|\n",
      "|  0|2023-08-26|        B+|Female|       B. Slater|       White English| LU8 1LK|     true|        true|       false|B_Slater|2023-08-...|\n",
      "|  0|2024-01-30|        O+|  Male|K. Krishnamurthy|               Mixed| BS2 7CT|    false|        true|       false|K_Krishnamurthy|2...|\n",
      "|  0|2023-11-03|        A+|Female|     F. Chambers|       White English| WV3 5YB|     true|        true|       false|F_Chambers|2023-1...|\n",
      "|  0|2024-01-01|        O+|  Male|  I. Q. M. Barry|      White Scottish| DN8 8JJ|    false|        true|       false|I_Q_M_Barry|2024-...|\n",
      "|  0|2023-11-01|        A+|  Male|     R. Stephens|       White English| TS8 7CN|    false|        true|       false|R_Stephens|2023-1...|\n",
      "|  0|2023-05-24|        A+|Female|        A. Allan|       White English|  L7 7IZ|     true|        true|       false|A_Allan|2023-05-2...|\n",
      "|  0|2023-08-24|        A-|Female|      G. Sridhar|               Mixed| KT9 0CN|     true|        true|       false|G_Sridhar|2023-08...|\n",
      "|  0|2024-03-08|        O+|Female|        Y. Jones|       White English| SE5 7LK|     true|        true|       false|Y_Jones|2024-03-0...|\n",
      "|  0|2023-09-04|        A-|Female|      D. G. Shah|       White English| ML1 4HY|     true|        true|       false|D_G_Shah|2023-09-...|\n",
      "|  0|2024-05-03|        B+|  Male|     O. Hamilton|       White English| CF3 0LF|    false|        true|       false|O_Hamilton|2024-0...|\n",
      "|  0|2024-04-11|        O+|  Male|      Q. Walters|         White other| CF7 8CE|    false|        true|       false|Q_Walters|2024-04...|\n",
      "|  0|2024-01-13|        A+|Female|     V. K. Evans|       White English| HS6 5ZF|     true|        true|       false|V_K_Evans|2024-01...|\n",
      "|  0|2023-12-06|        A+|  Male|     P. J. Giles|       White English|  B1 3SN|    false|        true|       false|P_J_Giles|2023-12...|\n",
      "|  0|2023-10-20|        B+|  Male|        S. Hogan|Mixed White/Black...| NE6 8LH|    false|        true|       false|S_Hogan|2023-10-2...|\n",
      "+---+----------+----------+------+----------------+--------------------+--------+---------+------------+------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T21:16:00.002061Z",
     "start_time": "2024-05-07T21:15:56.837462Z"
    }
   },
   "id": "c3be2c9b63747804"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot_age_distribution_with_sd(csv_age_uk_df, average_age)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b067d44ccbd2c1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot_kernel_density_age_distribution(csv_age_uk_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5e2fc658fbefcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt_df = base_df.repartition(8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e8d70235dde4f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13cde2969e689e96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "opt_df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"base_table_10000\")\n",
    "end = time.time()\n",
    "print(\"time taken to save df: \" + str(end - start))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "269b5ea1ecbc4515"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_table_df = spark.sql(\"select * from base_table_10000\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "241ae445badd7db9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_table_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f7a3a1cd07de2bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463e373907bf09e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.constants.admission_types_dataset import admission_mapping\n",
    "flattened = [\n",
    "    (\n",
    "        top_level.name,\n",
    "        sub_level_key.name,\n",
    "        list(sub_level_info.get(\"stay_types\")),\n",
    "        sub_level_info.get(\"tests\"),  \n",
    "        random.choice(sub_level_info.get(\"doctors\"))\n",
    "    )\n",
    "    for top_level, sub_level_dict in admission_mapping.items()\n",
    "    for sub_level_key, sub_level_info in sub_level_dict.items()\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa93b831b3aed7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76324a29d381e7c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.constants.condition_probabilities import condition_age_probability_dict\n",
    "\n",
    "flattened_condition_probabilities = [\n",
    "    (\n",
    "        sub_admission,\n",
    "        condition,\n",
    "        gender if gender in ['male', 'female'] else None,\n",
    "        float(age_range[0]),\n",
    "        float(age_range[1]) if len(age_range) > 1 else float('inf'),\n",
    "        float(probability) \n",
    "    )\n",
    "    for sub_admission, conditions in condition_age_probability_dict.items()\n",
    "    for condition, genders_or_age_prob_list in conditions.items()\n",
    "    for gender, age_prob_list in (genders_or_age_prob_list.items() if isinstance(genders_or_age_prob_list, dict) else [(None, genders_or_age_prob_list)])\n",
    "    for age_range, probability in age_prob_list\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "flattened_condition_probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d8352a033cdbc4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa988bdaed2265",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [\"condition_admission_type\", \"condition\", \"gender\", \"age_min\", \"age_max\", \"probability\"]\n",
    "schema = StructType([\n",
    "    StructField(\"condition_admission_type\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True),\n",
    "    StructField(\"condition_gender\", StringType(), True),  # Assuming gender can be specific probability for gender 'male', 'female', or null\n",
    "    StructField(\"age_min\", FloatType(), False),\n",
    "    StructField(\"age_max\", FloatType(), False),\n",
    "    StructField(\"probability\", FloatType(), False)\n",
    "])\n",
    "\n",
    "condition_prob_df = spark.createDataFrame(data=flattened_condition_probabilities, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a88bdeea4c696e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_prob_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc302cbd5cd49e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1 \n",
    "    From the flattened data list we wish to create a DataFrame. This contains all the possible combinations for the given \n",
    "    top level admissions, sub level admissions, stay types and list of tests available from the admission_mapping, stay_type and admission_tests lists or dictionaries in admission_types_test_dataset.py\n",
    "\"\"\"\n",
    "\n",
    "mapping_df = spark.createDataFrame(flattened, [\"top_level_admission\", \"sub_level_admission\", \"stay_types\", \"possible_tests\", \"doctor\"])\n",
    "\n",
    "joined_tbl = mapping_df.join(condition_prob_df, on=[mapping_df.sub_level_admission == condition_prob_df.condition_admission_type], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Type, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "\n",
    "class TaskWrapper:\n",
    "    def __init__(self, spark, task_class, *args, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            spark (SparkSession): The SparkSession\n",
    "            task_class: The class that's going to be run.\n",
    "            *args: The args associated with class.\n",
    "            **kwargs: The kwargs associated with class, if any.\n",
    "        \"\"\"\n",
    "        self.task = task_class(spark, *args, **kwargs)\n",
    "\n",
    "    def runner(self) -> Any:\n",
    "        \"\"\"\n",
    "        Calls the task runner function\n",
    "        Returns:\n",
    "            Any: returns Any type, right now DataFrame\n",
    "        \"\"\"\n",
    "        return self.task.runner()\n",
    "\n",
    "\n",
    "def process_partition(spark: SparkSession,\n",
    "                      partition_id: int,\n",
    "                      class_to_run: Type,\n",
    "                      df: DataFrame,\n",
    "                      *args,\n",
    "                      **kwargs) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This processes the partition result of the spark_partition process result from the TaskWrapper.runner()\n",
    "    Args:\n",
    "        spark (SparkSession): The SparkSession\n",
    "        partition_id (int): The partition id to filter against the df\n",
    "        class_to_run (Type): The class to run\n",
    "        df (DataFrame): The partitioned df \n",
    "        *args: The args associated with class.\n",
    "        **kwargs: The kwargs associated with class, if any.\n",
    "    Returns:\n",
    "        Dataframe: The Dataframe result. \n",
    "    \"\"\"\n",
    "    # Filter the DataFrame for this partition's data\n",
    "    partition_df = df.filter(spark_partition_id() == partition_id)\n",
    "\n",
    "    # Update class instantiation with *args and **kwargs\n",
    "    task = TaskWrapper(spark, class_to_run, partition_df, *args[1:], **kwargs)\n",
    "\n",
    "    result_df = task.runner()\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def run_multithreaded_processing(spark: SparkSession,\n",
    "                                 class_to_run: Type,\n",
    "                                 num_partitions: int,\n",
    "                                 df: DataFrame,\n",
    "                                 *args,\n",
    "                                 **kwargs) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Runs the Threaded Processing Process\n",
    "    Args:\n",
    "        spark (SparkSession): The SparkSession\n",
    "        class_to_run (Type): The class to run\n",
    "        num_partitions (int): total number of partitions to iterate over \n",
    "        df (DataFrame): The partitioned df \n",
    "        *args: The args associated with class.\n",
    "        **kwargs: The kwargs associated with class, if any.\n",
    "    Returns:\n",
    "       DataFrame: The unionised DataFrame\n",
    "    \"\"\"\n",
    "    partitioned_dfs = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_partitions) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_partition, spark, i, class_to_run, df, *args, **kwargs)\n",
    "            for i in range(num_partitions)\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result_df = future.result()\n",
    "            partitioned_dfs.append(result_df)\n",
    "\n",
    "    if partitioned_dfs:\n",
    "        final_df = reduce(DataFrame.unionByName, partitioned_dfs)\n",
    "        return final_df\n",
    "    return spark.createDataFrame([], args[0].schema)\n",
    "\n",
    "\n",
    "def runner(spark: SparkSession,\n",
    "           class_to_run: Type,\n",
    "           *args,\n",
    "           **kwargs) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        spark (SparkSession): The SparkSession\n",
    "        class_to_run (Type): The class to run\n",
    "        *args: The args associated with class.\n",
    "        **kwargs: The kwargs associated with class, if any.\n",
    "    Returns:\n",
    "        DataFrame: The returned DataFrame from the class to run.\n",
    "    \"\"\"\n",
    "    df = args[0]  # Assuming the first positional argument is always the DataFrame\n",
    "    total_rows = df.count()\n",
    "    rows_per_partition = 100\n",
    "    num_partitions = total_rows // rows_per_partition + (1 if total_rows % rows_per_partition else 0)\n",
    "    df = df.repartition(num_partitions)\n",
    "    return run_multithreaded_processing(spark, class_to_run, num_partitions, df, *args, **kwargs)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34b956be7f228b17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 2\n",
    "Create Enum class df and join it on to main driver df. \n",
    "Then create conditions \n",
    "\"\"\"\n",
    "\n",
    "from src.data_generator.conditions_creator import ConditionsCreator\n",
    "# from src.utils.thread_operations import runner\n",
    "from src.constants.type_constants import DepartmentTypes\n",
    "\n",
    "enum_values = [e.name for e in DepartmentTypes]\n",
    "\n",
    "enum_df = spark.createDataFrame(enum_values, StringType()).toDF(\"admission_type\")\n",
    "\n",
    "selected_conditions_df = runner(spark, ConditionsCreator, base_table_df, joined_tbl, enum_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "632611cc8f270e95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_conditions_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c10923969b8157e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_conditions_df = base_table_df.join(selected_conditions_df, on=\"unique_id\", how=\"left\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48659b8011e4036"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_conditions_df.show().where(\"unique_id\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "229cc4370c51623"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_table_df.filter(col(\"unique_id\") == \"T_Godfrey|2024-01-27\").show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0841a7c38981de5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "counts_df = new_conditions_df.groupBy(\"unique_id\").agg(count(\"unique_id\").alias(\"count\")).filter(col(\"count\") > 1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa391030e8ed351e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e561b5b626a11a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.util_funcs import get_row_count\n",
    "\n",
    "get_row_count(selected_conditions_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c3e16d26fc1e5f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_conditions_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c737cfa61730f556"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79379063057bfee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:  \n",
    "# tests to be chosen\n",
    "# admission date to be chosen \n",
    "# hospital name \n",
    "# patient postcode \n",
    "# ethnicity distribution  \n",
    "# _________________\n",
    "# Long term \n",
    "# investigation which disease affects specific ethnicities\n",
    "# blood type udf would need to consider ethnicity also eventually\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0099b41d9dc86",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
