{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from data_generator.csv_data_processor import CSVDataProcessor\n",
    "from utils.util_funcs import get_row_count, display_df, remove_data, verify_ranking, verify_ranking_counts\n",
    "from utils.read_write import read_postgres_table\n",
    "from spark_instance import spark\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.sql.functions import col, lower, lit, rand, array, concat, when, row_number, concat_ws, date_format, split, size\n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "344f364a0ecf68e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csv_reader = CSVDataProcessor(spark, \"data/healthcare_dataset.csv\")\n",
    "# \n",
    "# # Read the CSV file\n",
    "# raw_df = csv_reader.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# min_age_days = 1 * 365  # Minimum age in days (18 years)\n",
    "# max_age_days = 90 * 365  # Maximum age in days (90 years)\n",
    "# \n",
    "# raw_df = (raw_df.withColumn(\"DOB\", expr(f\"date_sub(current_date(), CAST(round(rand() * ({max_age_days} - {min_age_days}) + {min_age_days}) AS INT))\"))\n",
    "#         .withColumn(\"Age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25)))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62f17c7d47c49e7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# above is commented out as it's saved in the database:\n",
    "df = read_postgres_table(\"dob_age_raw_data\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19c3e561ca62f93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from constants.admission_types_dataset import admission_mapping, AdmissionTypes\n",
    "flattened = [\n",
    "    (\n",
    "        top_level.name,\n",
    "        sub_level_key.name,\n",
    "        list(sub_level_info.get(\"stay_types\")),\n",
    "        sub_level_info.get(\"tests\"),  \n",
    "        random.choice(sub_level_info.get(\"doctors\"))\n",
    "    )\n",
    "    for top_level, sub_level_dict in admission_mapping.items()\n",
    "    for sub_level_key, sub_level_info in sub_level_dict.items()\n",
    "]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d463e373907bf09e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flattened"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aa93b831b3aed7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from constants.condition_probabilities import condition_age_probability_dict\n",
    "\n",
    "flattened_condition_probabilities = [\n",
    "    (\n",
    "        sub_admission,\n",
    "        condition,\n",
    "        gender if gender in ['male', 'female'] else None,\n",
    "        float(age_range[0]),\n",
    "        float(age_range[1]) if len(age_range) > 1 else float('inf'),\n",
    "        float(probability) \n",
    "    )\n",
    "    for sub_admission, conditions in condition_age_probability_dict.items()\n",
    "    for condition, genders_or_age_prob_list in conditions.items()\n",
    "    for gender, age_prob_list in (genders_or_age_prob_list.items() if isinstance(genders_or_age_prob_list, dict) else [(None, genders_or_age_prob_list)])\n",
    "    for age_range, probability in age_prob_list\n",
    "]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76324a29d381e7c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "columns = [\"condition_admission_type\", \"condition\", \"gender\", \"age_min\", \"age_max\", \"probability\"]\n",
    "schema = StructType([\n",
    "    StructField(\"condition_admission_type\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True),\n",
    "    StructField(\"condition_gender\", StringType(), True),  # Assuming gender can be specific probability for gender 'male', 'female', or null\n",
    "    StructField(\"age_min\", FloatType(), False),\n",
    "    StructField(\"age_max\", FloatType(), False),\n",
    "    StructField(\"probability\", FloatType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "condition_prob_df = spark.createDataFrame(data=flattened_condition_probabilities, schema=schema)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18fa988bdaed2265"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "condition_prob_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76a88bdeea4c696e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c5cd63962527f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1 \n",
    "    From the flattened data list we wish to create a DataFrame. This contains all the possible combinations for the given \n",
    "    top level admissions, sub level admissions, stay types and list of tests available from the admission_mapping, stay_type and admission_tests lists or dictionaries in admission_types_test_dataset.py\n",
    "\"\"\"\n",
    "\n",
    "mapping_df = spark.createDataFrame(flattened, [\"top_level_admission\", \"sub_level_admission\", \"stay_types\", \"possible_tests\", \"doctor\"])\n",
    "\n",
    "display_df(mapping_df, 45)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1dc302cbd5cd49e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joined_tbl = mapping_df.join(condition_prob_df, on=[mapping_df.sub_level_admission == condition_prob_df.condition_admission_type], how=\"left\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7215090b2c80a8c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joined_tbl.show(n=2000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2efb5e38286a2598"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 2 \n",
    "    Create a list of admission_types randomly assign this to the original patient in the original data set, \n",
    "    whilst dropping the original admission_type column. Then join with mapping_df on top_level_admission col to give access to possible \n",
    "    conditions, mappings and so on.\n",
    "\"\"\"\n",
    "admission_type_names = [member.name for member in AdmissionTypes]\n",
    "\n",
    "keys_array = array([lit(name) for name in admission_type_names])\n",
    "\n",
    "# Define constants and conditions\n",
    "female_only = [\"MATERNITY\", 'obstetrics']\n",
    "is_female = lower(col('gender')) == 'female'\n",
    "is_pediatric = col(\"Age\") < 18\n",
    "is_geriatric = (col(\"Age\") >= 65)\n",
    "\n",
    "df = (df.withColumn(\"is_female\", is_female)\n",
    "        .withColumn(\"is_pediatric\", is_pediatric)\n",
    "        .withColumn(\"is_geriatric\", is_geriatric)\n",
    "        .withColumn(\"unique_id\", concat_ws(\"_\", \"name\", date_format(\"DOB\", \"yyyyMMdd\")))\n",
    "        .drop(\"doctor\", \"medical_condition\", \"test_results\", \"medication\", \"admission_type\")\n",
    "      )\n",
    "get_row_count(df, True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b51112bb4b4cff7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from constants.admission_types_dataset import SubAdmissionTypes\n",
    "\"\"\"\n",
    "Create Enum class df and join it on to main driver df. \n",
    "\"\"\"\n",
    "enum_values = [e.name for e in SubAdmissionTypes]\n",
    "\n",
    "enum_df = spark.createDataFrame(enum_values, StringType()).toDF(\"admission_type\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3970b6d3692be37f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_enum_cross = df.crossJoin(enum_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c6d413eaa24d192"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_enum_cross.where((col(\"unique_id\") == \"Aaron Jones_20141222\") & (col(\"admission_type\") == \"CARDIOLOGY\")).show(n=2000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3f13e84e89d45fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_joined_temp = df_enum_cross.join(joined_tbl, df_enum_cross.admission_type == joined_tbl.condition_admission_type, how=\"left\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f22167b30cbed1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_joined_temp = df_joined_temp.where((col(\"unique_id\") == \"Aaron Jones_20141222\") & (col(\"admission_type\") == \"CARDIOLOGY\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c68fbb243749e52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from typing import List, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from IPython.core.display_functions import display\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, count, lit, when, rand, struct, collect_list, lit, udf\n",
    "from faker import Faker\n",
    "from collections import defaultdict\n",
    "from spark_instance import spark\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "from constants.type_constants import SubAdmissionTypes\n",
    "\n",
    "\n",
    "def find_probability_for_age_gender(age: int,\n",
    "                                    condition_probabilities: Tuple[Tuple[int, int], float]) -> float | int:\n",
    "    \"\"\"\n",
    "    This function returns probability based on age and a given condition probability list.\n",
    "    Args:\n",
    "        age: The age we will be comparing to\n",
    "        condition_probabilities: The list of conditional probabilities\n",
    "\n",
    "    Returns:\n",
    "        int: the probability, if 0 returns, edge case, should be investigated.\n",
    "    \"\"\"\n",
    "    (age_min, age_max), prob = condition_probabilities\n",
    "    if age_min <= age <= age_max:\n",
    "        return prob\n",
    "    return 0\n",
    "\n",
    "\n",
    "def filter_female_conditions(f_df):\n",
    "    \"\"\"\n",
    "    Filters out entries from a DataFrame based on gender and age criteria:\n",
    "    - Excludes female-only conditions (MATERNITY, OBSTETRICS) for non-female subjects.\n",
    "    - Applies age restrictions specifically for MATERNITY-related entries.\n",
    "    - Filter pediatric patients who cannot be pregnant (based on legal age in the UK, 16)\n",
    "      No assumption made an individual cannot choose to get pregnant before this age.\n",
    "      upper age bound defined  here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4822427/#:~:text=Menopause%20typically%20occurs%20between%2045,reproducing%20many%20years%20before%20menopause.\n",
    "      between 45 and 55 upper bounding will be 50.\n",
    "      # TODO Possibility to include outliers in a new func in the future.\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame with condition and demographic data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    female_only = [SubAdmissionTypes.MATERNITY.name, SubAdmissionTypes.OBSTETRICS.name]  #  TODO Obstetrics needs sorting still.\n",
    "\n",
    "    filtered_df = f_df.filter((~(col(\"top_level_admission\").isin(female_only))) & (~col(\"is_female\")))\n",
    "\n",
    "    filtered_df = filtered_df.withColumn(\"Age_Filter\",\n",
    "                       when((col(\"top_level_admission\") == SubAdmissionTypes.MATERNITY.name) &\n",
    "                            ((col(\"Age\") < 16) | (col(\"Age\") > 50)),\n",
    "                            False).otherwise(True))\n",
    "    \n",
    "    # Filter rows based on the Age_Filter col\n",
    "    filtered_df =  filtered_df.filter(col(\"Age_Filter\")).drop(\"Age_Filter\")\n",
    "    \n",
    "    return filtered_df.filter(\n",
    "                     (~col(\"is_female\") & (col(\"condition_gender\") != \"female\")) | \n",
    "                     (col(\"condition_gender\").isNull()) | col(\"is_female\")\n",
    "                     )\n",
    "    \n",
    "def filter_geriatric_conditions(g_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This function filters all GERIATRIC submission types\n",
    "    Args:\n",
    "        df (DataFrame): The unfiltered dataframe\n",
    "\n",
    "    Returns:\n",
    "        df (DataFrame): The filtered dataframe\n",
    "    \"\"\"\n",
    "    return g_df.filter(~(col(\"sub_level_admission\") == SubAdmissionTypes.GERIATRICS.name) & ~col(\"is_geriatric\"))\n",
    "\n",
    "\n",
    "def check_age_probability(age, age_min, age_max, probability):\n",
    "    if probability > 0 and age_min <= age <= age_max:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def choose_condition_for_patient(probability_df: DataFrame,\n",
    "                                 driver_df: DataFrame\n",
    "                                 ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This function chooses a condition for a patient based on the age and looking at the condition_age_probability_dict\n",
    "    Args:\n",
    "        probability_df (DataFrame):\n",
    "        driver_df (DataFrame):\n",
    "    Returns:\n",
    "        A condition that will be assigned to the patient or None if issues found.\n",
    "    \"\"\"\n",
    "    df_joined = driver_df.join(probability_df, driver_df.admission_type == probability_df.condition_admission_type, how=\"inner\")\n",
    "    \n",
    "    df_filtered = df_joined.filter((col(\"age\") >= col(\"age_min\")) & (col(\"age\") <= col(\"age_max\")))\n",
    "\n",
    "    df_filtered_female = filter_female_conditions(df_filtered)\n",
    "    df_filtered_geriatric = filter_geriatric_conditions(df_filtered_female)\n",
    "\n",
    "    df_ordered = df_filtered_geriatric.orderBy(col(\"unique_id\"))\n",
    "    \n",
    "    check_age_probability_udf = udf(check_age_probability, BooleanType())\n",
    "\n",
    "    df_checked_probability = df_ordered.withColumn(\"valid_probability\", check_age_probability_udf(\"Age\", \"age_min\", \"age_max\", \"probability\"))\n",
    " \n",
    "    \n",
    "    df_transformed = (df_checked_probability.withColumn(\"probability_entry\", \n",
    "                                        when(col(\"valid_probability\"), \n",
    "                                             struct(col(\"age_min\"), col(\"age_max\"), col(\"probability\"))\n",
    "                                        )\n",
    "                                ).withColumn(\"row_info\", \n",
    "                                        when(col(\"valid_probability\"),\n",
    "                                             struct(\n",
    "                                                col(\"Age\"),\n",
    "                                                col(\"condition\"),\n",
    "                                                col(\"is_pediatric\"),\n",
    "                                                col(\"unique_id\"),\n",
    "                                                col(\"condition_admission_type\"),\n",
    "                                                col(\"top_level_admission\")\n",
    "                                            )\n",
    "                                        )\n",
    "                                ).filter(col(\"valid_probability\") == True)\n",
    "                      )\n",
    "    \n",
    "    df_aggregated = df_transformed.groupBy(\"unique_id\").agg(collect_list(\"probability_entry\").alias(\"probability_entries\"),\n",
    "                                                            collect_list(\"row_info\").alias(\"row_infos\"))\n",
    "\n",
    "    conditions_probabilities = []\n",
    "    patients_conditions = defaultdict(list)\n",
    "    for row in df_aggregated.toLocalIterator():\n",
    "        unique_id = row[\"unique_id\"]\n",
    "        probability_entries = [((entry.age_min, entry.age_max), entry.probability) for entry in row[\"probability_entries\"]]\n",
    "        row_infos = [(entry.Age, entry.condition, entry.is_pediatric, unique_id, entry.condition_admission_type, entry.top_level_admission) for entry in row[\"row_infos\"]]\n",
    "        for probability_info, patient_info in zip(probability_entries, row_infos):\n",
    "            age, condition, is_pediatric, unique_id, condition_admission_type, top_level_admission = patient_info\n",
    "            age_prob = probability_info\n",
    "    \n",
    "            prob = find_probability_for_age_gender(age, age_prob)\n",
    "            if prob > 0:\n",
    "                condition_label = f\"pediatric_{condition}\" if is_pediatric else condition\n",
    "            else:\n",
    "                condition_label = \"pediatric no condition for patient edge case\" if is_pediatric else \"no condition for patient edge case\"\n",
    "                prob = 0\n",
    "    \n",
    "            conditions_probabilities.append((f\"{unique_id}_{top_level_admission}_{condition_admission_type}\", condition_label, prob))\n",
    "            patient_name = unique_id.split('_')[0]\n",
    "            patients_conditions[patient_name].append((unique_id, top_level_admission, condition_label, prob))\n",
    "\n",
    "\n",
    "    # Sort the conditions by probability for easier handling (optional)\n",
    "    conditions_probabilities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Step 2: Select a condition for each patient\n",
    "    chosen_conditions_list = []\n",
    "    for patient, conditions in patients_conditions.items():\n",
    "        total_prob = sum(prob for _, _, prob in conditions_probabilities)\n",
    "        condition_selected = False\n",
    "        while not condition_selected:\n",
    "            random_prob = np.random.uniform(0, total_prob)\n",
    "            cumulative_prob = 0\n",
    "            for uniq_id, top_level, condition, prob in conditions:\n",
    "                cumulative_prob += prob\n",
    "                if random_prob < cumulative_prob:\n",
    "                    chosen_conditions_list.append((uniq_id, top_level, condition))\n",
    "                    condition_selected = True\n",
    "                    break   # Stop after selecting one condition for the current patient\n",
    "    return spark.createDataFrame(chosen_conditions_list, (\"unique_id\", \"chosen_top_level_admission\", \"chosen_condition\"))\n",
    "    \n",
    "                \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0bbf2e4aa077ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import ast\n",
    "# tuples_list = []\n",
    "# \n",
    "# with open(\"end_list.txt\", \"r\") as f:\n",
    "#     for line in f:\n",
    "#         # Use ast.literal_eval() to safely evaluate the string as a Python literal\n",
    "#         # Assuming each line in the file contains a tuple\n",
    "#         evaluated_tuple = ast.literal_eval(line.strip())\n",
    "#         tuples_list.append(evaluated_tuple)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acfd1c98073120f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for v in tuples_list:\n",
    "#     \n",
    "#     for k in v:\n",
    "#         print(k)\n",
    "#         count += 1\n",
    "# print(count)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64126c96d17ea3f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_con = choose_condition_for_patient(joined_tbl, df_enum_cross)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d43cbad52e5edd6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.read_write import write_postgres_table"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f9d4534a513bd35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from data_generator.constants import properties, POSTGRES_URL\n",
    "\n",
    "df_con.write.jdbc(url=POSTGRES_URL, table=\"chosen_conditions_2503\", mode=\"overwrite\", properties=properties)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f57ff0ce18838d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 3 \n",
    "    Create a row ranking using a unique cols, created from stay_name and unique_id. \n",
    "    Verification done below. \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "df =  df.withColumn('stay_name', \n",
    "                   when(col('stay_type') == 'out_patient', concat(col('name'), lit('_out_patient')))\n",
    "                   .when(col('stay_type') == 'inpatient', concat(col('name'), lit('_inpatient')))\n",
    "                   .when(col('stay_type') == 'day_patient', concat(col('name'), lit('_day_patient')))\n",
    "                   .otherwise(col('name'))\n",
    "                  )\n",
    "\n",
    "\n",
    "# Define a window specification that partitions data by 'top_level_admission' (or another unique patient identifier if needed)\n",
    "windowSpec = Window.partitionBy('stay_name', 'unique_id').orderBy(rand())\n",
    "\n",
    "# Assign row numbers within each partition in a random order\n",
    "ranked_df = df.withColumn(\"row_num\", row_number().over(windowSpec))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e039e0ce5ff0816d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranked_df.createGlobalTempView(\"ranked_df\")\n",
    "\n",
    "unique_dobs_df = spark.sql(\"\"\"\n",
    "WITH NameCounts AS (\n",
    "    SELECT name\n",
    "    FROM global_temp.ranked_df\n",
    ")\n",
    "\n",
    "SELECT DISTINCT r.name, r.unique_id\n",
    "FROM global_temp.ranked_df r\n",
    "JOIN NameCounts n ON r.name = n.name\n",
    "ORDER BY r.name\n",
    "LIMIT 10\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e16046564d50a89a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# COSTLY WAY TO VERIFY THAT THE RANK WORKS BELOW....  due to .collect() and .count() in verify_ranking_counts() function\n",
    "\n",
    "# Example usage:\n",
    "unique_names = [row['name'] for row in unique_dobs_df.select(\"name\").collect()]\n",
    "unique_ids = [row['unique_id'] for row in unique_dobs_df.select(\"unique_id\").collect()]\n",
    "verify_ranking_counts(df, ranked_df, unique_names, unique_ids)\n",
    "\n",
    "# this function is faster than above\n",
    "verify_ranking(df, ranked_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8a7e1a934ab2401"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 4: \n",
    "    Sort out geriatrics data and verify that individuals are not geriatrics \n",
    "\"\"\"\n",
    "ranked_df = ranked_df.withColumn(\"is_geriatric\", is_geriatric)\n",
    "\n",
    "not_geriatric_df = ranked_df.where((col('sub_level_admission') == \"GERIATRICS\") &( col(\"is_geriatric\") == False))\n",
    "\n",
    "filtered_df = remove_data(ranked_df, not_geriatric_df, (col('sub_level_admission') == \"GERIATRICS\"), ( col(\"is_geriatric\") == False))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9746b1d222d3d1ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5: \n",
    "    Sort out geriatrics data and verify that individuals who are not female and should not have a female sub level admission \n",
    "\"\"\"\n",
    "\n",
    "not_female_df = filtered_df.where((col(\"sub_level_admission\").isin(female_only)) & (col(\"is_female\") == False))\n",
    "filtered_df_female = remove_data(filtered_df, not_female_df, (col(\"gender\") == \"Male\"),  (col(\"sub_level_admission\").isin(female_only)))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7c1ee011b044991"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filter pediatric patients who cannot be pregnant (based on legal age in the UK, 16) No assumption made an individual cannot choose to get pregnant before this age. \n",
    "# upper age bound defined  here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4822427/#:~:text=Menopause%20typically%20occurs%20between%2045,reproducing%20many%20years%20before%20menopause.\n",
    "# between 45 and 55 upper bounding will be 50. # Possiblilty to include outliers in a new func in the future.\n",
    "under16_female_df = filtered_df_female.where((col(\"Age\") < 16) &\n",
    "                                             (col(\"Age\") <= 50) &\n",
    "                                             (col(\"sub_level_admission\") == \"MATERNITY\")\n",
    "                                             ).orderBy(\"Age\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "630396c288fa291b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = remove_data(filtered_df_female, under16_female_df, (col(\"sub_level_admission\") == \"MATERNITY\"),\n",
    "                 (col(\"Age\") < 16) & (col(\"Age\") > 50))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc11c7ec6b0a65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "split_col = split(df['name'], ' ')\n",
    "\n",
    "df_renamed = (df.withColumn('name', \n",
    "                    when(size(split_col) == 4, concat(split_col.getItem(1).substr(1, 1), lit('. '), split_col.getItem(2).substr(1, 1), lit('. '), split_col.getItem(3)))\n",
    "                    .when(size(split_col) == 3, concat(split_col.getItem(1).substr(1, 1), lit('. '), split_col.getItem(2)))\n",
    "                    .otherwise(\n",
    "                        concat(split_col.getItem(0).substr(1, 1), lit('. '), split_col.getItem(1))\n",
    "                               )\n",
    "                    )\n",
    "      )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d883931ab7588b0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_renamed.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3bb25e7bfae988"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_renamed.select(\"name\", \"DOB\", \"Age\", \"gender\", \"blood_type\", \"date_of_admission\", \"discharge_date\", \"top_level_admission\", \"sub_level_admission\", \"possible_tests\", \"conditions\", \"doctor\", \"hospital\", \"room_number\", \"insurance_provider\", \"billing_amount\", \"stay_type\", \"is_female\", \"is_geriatric\", \"is_pediatric\", \"stay_name\", \"row_num\", \"unique_id\")\n",
    " # .where(col(\"stay_name\") ==\"Abigail Lamb\")\n",
    " .show(n=200))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d88ea8c674b90792"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "sub_selection_cols = [\"Age\", \"sub_level_admission\", \"conditions\", \"row_num\", \"unique_id\"]\n",
    "\n",
    "# df_subset = df_renamed.select(*sub_selection_cols)\n",
    "# \n",
    "# choose_condition_udf = udf(choose_condition, StringType())\n",
    "# df_test = df_subset.withColumn(\"assigned_condition\", choose_condition_udf(df_subset[\"age\"], df_subset[\"sub_level_admission\"], df_subset[\"conditions\"]))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9577664b552b65c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_subset.persist()\n",
    "# df_subset.show()  # The action that triggers the computation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c30c752422e5e8c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_test.persist()\n",
    "# df_test.show(n=20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d4371ffc767013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_new = df_renamed.drop(\"row_num\")\n",
    "\n",
    "windowSpec = Window.partitionBy('stay_name', 'unique_id').orderBy(rand())\n",
    "\n",
    "# Assign row numbers within each partition in a random order\n",
    "df_new_part = df_new.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "\n",
    "df_new_part.select([\"name\", \"DOB\", \"Age\", \"gender\", \"blood_type\", \"date_of_admission\", \"discharge_date\", \"top_level_admission\", \"sub_level_admission\", \"possible_tests\", \"conditions\", \"doctor\", \"hospital\", \"room_number\", \"insurance_provider\", \"billing_amount\", \"stay_type\", \"is_female\", \"is_geriatric\", \"is_pediatric\", \"stay_name\", \"row_num\", \"unique_id\"]).where(col(\"stay_name\") ==\"Tiffany Ramirez\").show() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc7aa0cdf857cd7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_row_count(df_new_part)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e88430e60903a00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[\"name\", \"DOB\", \"Age\", \"gender\", \"blood_type\", \"date_of_admission\", \"discharge_date\", \"top_level_admission\", \"sub_level_admission\", \"possible_tests\", \"conditions\", \"doctor\", \"hospital\", \"room_number\", \"insurance_provider\", \"billing_amount\", \"stay_type\", \"is_female\", \"is_geriatric\" \"is_pediatric\", \"stay_name\", \"row_num\", \"unique_id\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5763dee08b19e6d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# join_with_condition_prob_df = df_new_part.join(condition_prob_df, \n",
    "#                                                on=[\"sub_level_admission\"], \n",
    "#                                                how=\"inner\")\n",
    "# join_with_condition_prob_df = join_with_condition_prob_df.filter(\n",
    "#     (col(\"Age\") >= col(\"age_min\")) &\n",
    "#     (col(\"Age\") <= col(\"age_max\"))\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea69f799f30eb66b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_new_part.show(n=2000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a171c1b405d88408"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO:  \n",
    "# medical condition to be  chosen \n",
    "# tests to be chosen\n",
    "# admission date to be checked again dob,  \n",
    "# TODO filter on is pediatric, geriatric and is_female to be done here and same people with dob? needs  to be considered \n",
    "# drop stay_name and unique_id \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a79379063057bfee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter to keep only the top-ranked row within each partition\n",
    "ranked_df = filtered_df_female"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4879749b20208b88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StringType\n",
    "# get_row_count(df, True)\n",
    "df = join_with_condition_prob_df.select([col(c).cast(StringType()).alias(c) for c in join_with_condition_prob_df.columns])\n",
    "df.show()\n",
    "df.repartition(10).write.csv('./temp_data/join_with_condition_prob_df/renamed.csv', mode = 'overwrite', header=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eb59205ef488cec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO initialise first name to make data more realistic for name columns i.e. Daniel Mccoy is seen a Male name but here its Female"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31a9a6e7e0527585"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88f0099b41d9dc86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dict_ = admission_mapping.get(\"emergency\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e68e3f5c6b955395"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d986e68bfc10315d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "46d89b5173e46238"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
