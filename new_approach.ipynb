{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f364a0ecf68e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from data_generator.csv_data_processor import CSVDataProcessor\n",
    "from utils.util_funcs import get_row_count, display_df, remove_data, verify_ranking, verify_ranking_counts\n",
    "from utils.read_write import read_postgres_table\n",
    "from spark_instance import spark\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.sql.functions import col, lower, rand, array,row_number, concat_ws, date_format, sum, udf\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_age = 40.1\n",
    "\n",
    "csv_age_file_file = CSVDataProcessor(spark, \"data/uk_age_population.csv\")\n",
    "\n",
    "csv_age_uk_df = csv_age_file_file.runner()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48c373a8586c425e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_age_uk_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3be2c9b63747804"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the squared differences from the known mean, weighted by the population total\n",
    "csv_age_uk_sq_df = csv_age_uk_df.withColumn(\"weighted_squared_diff\", (col(\"age\") - 40.2) ** 2 * col(\"population_total\"))\n",
    "\n",
    "# Sum up the weighted squared differences and the total population\n",
    "total_weighted_squared_diff = csv_age_uk_sq_df.select(sum(\"weighted_squared_diff\")).collect()[0][0]\n",
    "total_population = csv_age_uk_sq_df.select(sum(\"population_total\")).collect()[0][0]\n",
    "\n",
    "# Calculate the weighted variance\n",
    "weighted_variance = total_weighted_squared_diff / total_population\n",
    "\n",
    "# Calculate the weighted standard deviation\n",
    "weighted_standard_deviation = math.sqrt(weighted_variance)\n",
    "\n",
    "print(\"Weighted Standard Deviation of Age:\", weighted_standard_deviation)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3df6f1d125d889f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ff7fa58dd799cd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pandas_df = csv_age_uk_sq_df.toPandas()\n",
    "\n",
    "mean_age = 40.2\n",
    "\n",
    "pandas_df['age'] = pandas_df['age'].astype(float)\n",
    "pandas_df['population_total'] = pandas_df['population_total'].astype(int)\n",
    "pandas_df['weighted_squared_diff'] = pandas_df['population_total'] * (pandas_df['age'] - mean_age) ** 2\n",
    "\n",
    "total_weighted_squared_diff = np.sum(pandas_df['weighted_squared_diff'])\n",
    "total_population = np.sum(pandas_df['population_total'])\n",
    "\n",
    "weighted_variance = total_weighted_squared_diff / total_population\n",
    "standard_deviation = np.sqrt(weighted_variance)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(pandas_df, x='age', weights='population_total', bins=range(int(pandas_df['age'].min()), int(pandas_df['age'].max()) + 1), color='skyblue', kde=False)\n",
    "\n",
    "plt.axvline(mean_age, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(mean_age + standard_deviation, color='green', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(mean_age - standard_deviation, color='green', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.axvspan(float(mean_age) - standard_deviation, float(mean_age) + standard_deviation, alpha=0.1, color='green')\n",
    "\n",
    "plt.title('Age Distribution with Standard Deviation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Population Total')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b067d44ccbd2c1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the KDE plot\n",
    "sns.kdeplot(data=pandas_df, x='age', weights='population_total', fill=True, common_norm=False, bw_adjust=0.5, clip=(pandas_df['age'].min(), pandas_df['age'].max()))\n",
    "\n",
    "plt.axvline(x=40.2, color='r', linestyle='--')\n",
    "\n",
    "plt.title('Kernel Density Estimate of Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5e2fc658fbefcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "total_population = csv_age_uk_sq_df.select(sum(\"population_total\")).collect()[0][0]\n",
    "\n",
    "\n",
    "csv_age_uk_sq_df = csv_age_uk_sq_df.withColumn(\"density\", col(\"population_total\") / lit(total_population))\n",
    "\n",
    "# Normalize the density to ensure it sums to 1\n",
    "total_density = csv_age_uk_sq_df.select(sum(\"density\")).collect()[0][0]\n",
    "new_csv_age_uk_sq_df = csv_age_uk_sq_df.withColumn(\"normalized_density\", col(\"density\") / lit(total_density))\n",
    "\n",
    "\n",
    "num_samples = 1000000\n",
    "# Oversample by a small percentage, this is to take care of rounding errors in the system\n",
    "oversample_factor = 1.1  \n",
    "oversample_num = int(num_samples * oversample_factor)\n",
    "sampled_rdd = new_csv_age_uk_sq_df.rdd.flatMap(\n",
    "    lambda row: [row['age']] * int(row['normalized_density'] * oversample_num)\n",
    ")\n",
    "\n",
    "row_rdd = sampled_rdd.map(lambda age: Row(Age=age))\n",
    "sampled_df = spark.createDataFrame(row_rdd)\n",
    "\n",
    "print(sampled_df.count())\n",
    "sampled_df = sampled_df.orderBy(rand()).limit(num_samples)\n",
    "\n",
    "print(sampled_df.count())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbc8ab960c79ce19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71e065f8031e4003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.pandas.functions import pandas_udf\n",
    "from pyspark.sql.types import  DateType, StringType\n",
    "\n",
    "\n",
    "@pandas_udf(DateType())\n",
    "def create_random_dob_pandas_udf(ages: pd.Series) -> pd.Series:\n",
    "    today = pd.Timestamp('today').normalize()\n",
    "    \n",
    "    preliminary_dobs = today - pd.to_timedelta(ages * 365, unit='d')\n",
    "    random_days = np.random.randint(0, 365, size=len(ages))\n",
    "    # Calculate the final DOB\n",
    "    final_dobs = preliminary_dobs - pd.to_timedelta(random_days, unit='d')\n",
    "    return final_dobs\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "868143a4f536ee47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from random import choices\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def create_fake_name_udf(size: pd.Series) -> pd.Series: \n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        size: \n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    fake = Faker()\n",
    "    names = []\n",
    "    for _ in range(len(size)):\n",
    "        initials_count = choices([1, 2, 3], weights=[1, 0.5, 0.2], k=1)[0]\n",
    "        initials = ' '.join([f\"{fake.random_uppercase_letter()}.\" for _ in range(initials_count)])\n",
    "        surname = fake.last_name()\n",
    "        names.append(f\"{initials} {surname}\")\n",
    "    return pd.Series(names)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da717741baef6608"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_with_dob = sampled_df.withColumn(\"DOB\", create_random_dob_pandas_udf(col(\"Age\")))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25427fd3ebc1a743"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_with_dob.show(n=1000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b42f6d2f297b50f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_with_dob = sampled_df_with_dob.withColumn(\"dummy\", lit(1))\n",
    "sampled_df_with_name = sampled_df_with_dob.withColumn(\"name\", create_fake_name_udf(col(\"dummy\"))).drop(\"dummy\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f87bda80e7f65562"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_with_name.show(n=20000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a562edd7a9772e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# csv_reader = CSVDataProcessor(spark, \"data/healthcare_dataset.csv\")\n",
    "# \n",
    "# # Read the CSV file\n",
    "# raw_df = csv_reader.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f17c7d47c49e7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# min_age_days = 1 * 365  # Minimum age in days (18 years)\n",
    "# max_age_days = 90 * 365  # Maximum age in days (90 years)\n",
    "# \n",
    "# raw_df = (raw_df.withColumn(\"DOB\", expr(f\"date_sub(current_date(), CAST(round(rand() * ({max_age_days} - {min_age_days}) + {min_age_days}) AS INT))\"))\n",
    "#         .withColumn(\"Age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3e561ca62f93",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# above is commented out as it's saved in the database:\n",
    "df = read_postgres_table(\"dob_age_raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463e373907bf09e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from constants.admission_types_dataset import admission_mapping, AdmissionTypes\n",
    "flattened = [\n",
    "    (\n",
    "        top_level.name,\n",
    "        sub_level_key.name,\n",
    "        list(sub_level_info.get(\"stay_types\")),\n",
    "        sub_level_info.get(\"tests\"),  \n",
    "        random.choice(sub_level_info.get(\"doctors\"))\n",
    "    )\n",
    "    for top_level, sub_level_dict in admission_mapping.items()\n",
    "    for sub_level_key, sub_level_info in sub_level_dict.items()\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa93b831b3aed7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76324a29d381e7c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from constants.condition_probabilities import condition_age_probability_dict\n",
    "\n",
    "flattened_condition_probabilities = [\n",
    "    (\n",
    "        sub_admission,\n",
    "        condition,\n",
    "        gender if gender in ['male', 'female'] else None,\n",
    "        float(age_range[0]),\n",
    "        float(age_range[1]) if len(age_range) > 1 else float('inf'),\n",
    "        float(probability) \n",
    "    )\n",
    "    for sub_admission, conditions in condition_age_probability_dict.items()\n",
    "    for condition, genders_or_age_prob_list in conditions.items()\n",
    "    for gender, age_prob_list in (genders_or_age_prob_list.items() if isinstance(genders_or_age_prob_list, dict) else [(None, genders_or_age_prob_list)])\n",
    "    for age_range, probability in age_prob_list\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flattened_condition_probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d8352a033cdbc4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa988bdaed2265",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "columns = [\"condition_admission_type\", \"condition\", \"gender\", \"age_min\", \"age_max\", \"probability\"]\n",
    "schema = StructType([\n",
    "    StructField(\"condition_admission_type\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True),\n",
    "    StructField(\"condition_gender\", StringType(), True),  # Assuming gender can be specific probability for gender 'male', 'female', or null\n",
    "    StructField(\"age_min\", FloatType(), False),\n",
    "    StructField(\"age_max\", FloatType(), False),\n",
    "    StructField(\"probability\", FloatType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "condition_prob_df = spark.createDataFrame(data=flattened_condition_probabilities, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a88bdeea4c696e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_prob_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5cd63962527f8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc302cbd5cd49e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1 \n",
    "    From the flattened data list we wish to create a DataFrame. This contains all the possible combinations for the given \n",
    "    top level admissions, sub level admissions, stay types and list of tests available from the admission_mapping, stay_type and admission_tests lists or dictionaries in admission_types_test_dataset.py\n",
    "\"\"\"\n",
    "\n",
    "mapping_df = spark.createDataFrame(flattened, [\"top_level_admission\", \"sub_level_admission\", \"stay_types\", \"possible_tests\", \"doctor\"])\n",
    "\n",
    "joined_tbl = mapping_df.join(condition_prob_df, on=[mapping_df.sub_level_admission == condition_prob_df.condition_admission_type], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51112bb4b4cff7a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 2 \n",
    "    Create a list of admission_types randomly assign this to the original patient in the original data set, \n",
    "    whilst dropping the original admission_type column. Then join with mapping_df on top_level_admission col to give access to possible \n",
    "    conditions, mappings and so on.\n",
    "\"\"\"\n",
    "admission_type_names = [member.name for member in AdmissionTypes]\n",
    "\n",
    "keys_array = array([lit(name) for name in admission_type_names])\n",
    "\n",
    "# Define constants and conditions\n",
    "female_only = [\"MATERNITY\", 'obstetrics']\n",
    "is_female = lower(col('gender')) == 'female'\n",
    "is_pediatric = col(\"Age\") < 18\n",
    "is_geriatric = (col(\"Age\") >= 65)\n",
    "\n",
    "df = (df.withColumn(\"is_female\", is_female)\n",
    "        .withColumn(\"is_pediatric\", is_pediatric)\n",
    "        .withColumn(\"is_geriatric\", is_geriatric)\n",
    "        .withColumn(\"unique_id\", concat_ws(\"_\", \"name\", date_format(\"DOB\", \"yyyyMMdd\")))\n",
    "        .drop(\"doctor\", \"medical_condition\", \"test_results\", \"medication\", \"admission_type\")\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970b6d3692be37f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 3  \n",
    "Create Enum class df and join it on to main driver df. \n",
    "Then create conditions \n",
    "\"\"\"\n",
    "\n",
    "from data_generator.conditions_creator import ConditionsCreator\n",
    "from utils.thread_operations import runner\n",
    "from constants.type_constants import SubAdmissionTypes\n",
    "\n",
    "enum_values = [e.name for e in SubAdmissionTypes]\n",
    "\n",
    "enum_df = spark.createDataFrame(enum_values, StringType()).toDF(\"admission_type\")\n",
    "\n",
    "\n",
    "\n",
    "selected_conditions_df = runner(spark, ConditionsCreator, df, joined_tbl, enum_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_count = selected_conditions_df.count()\n",
    "\n",
    "distinct_count = selected_conditions_df.select(\"unique_id\").distinct().count()\n",
    "\n",
    "is_unique = total_count == distinct_count\n",
    "\n",
    "print(f\"final_df is distinct for every row? {is_unique}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d17f37445f91f96a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get_row_count(selected_conditions_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "856b7cc736e333c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf363f1da4a0f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# selected_conditions_df.orderBy(col(\"unique_id\")).show(n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d496c5cbbf43c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_selected_conditions_df = df.join(selected_conditions_df, on=\"unique_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e290dbecadac6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_selected_conditions_df.where(col(\"chosen_condition\").isNull()).show(n=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984ebcf31c80767",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_row_count(joined_selected_conditions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.util_funcs import create_fake_name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b705c50a6213776"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883931ab7588b0d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_fake_name_udf = udf(create_fake_name, StringType())\n",
    "\n",
    "joined_selected_conditions_df = joined_selected_conditions_df.drop(\"name\")\n",
    "\n",
    "joined_selected_conditions_df = joined_selected_conditions_df.withColumn(\"new_name\", create_fake_name_udf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb25e7bfae988",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_selected_conditions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ea8c674b90792",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df_renamed.select(\"name\", \"DOB\", \"Age\", \"gender\", \"blood_type\", \"date_of_admission\", \"discharge_date\", \"top_level_admission\", \"sub_level_admission\", \"possible_tests\", \"conditions\", \"doctor\", \"hospital\", \"room_number\", \"insurance_provider\", \"billing_amount\", \"stay_type\", \"is_female\", \"is_geriatric\", \"is_pediatric\", \"stay_name\", \"row_num\", \"unique_id\")\n",
    " .show(n=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7aa0cdf857cd7c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = df_renamed.drop(\"row_num\")\n",
    "\n",
    "windowSpec = Window.partitionBy('stay_name', 'unique_id').orderBy(rand())\n",
    "\n",
    "# Assign row numbers within each partition in a random order\n",
    "df_new_part = df_new.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "\n",
    "df_new_part.select([\"name\", \"DOB\", \"Age\", \"gender\", \"blood_type\", \"date_of_admission\", \"discharge_date\", \"top_level_admission\", \"sub_level_admission\", \"possible_tests\", \"conditions\", \"doctor\", \"hospital\", \"room_number\", \"insurance_provider\", \"billing_amount\", \"stay_type\", \"is_female\", \"is_geriatric\", \"is_pediatric\", \"stay_name\", \"row_num\", \"unique_id\"]).where(col(\"stay_name\") ==\"Tiffany Ramirez\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88430e60903a00",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_row_count(df_new_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763dee08b19e6d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[\"name\", \"DOB\", \"Age\", \"gender\", \"blood_type\", \"date_of_admission\", \"discharge_date\", \"top_level_admission\", \"sub_level_admission\", \"possible_tests\", \"conditions\", \"doctor\", \"hospital\", \"room_number\", \"insurance_provider\", \"billing_amount\", \"stay_type\", \"is_female\", \"is_geriatric\" \"is_pediatric\", \"stay_name\", \"row_num\", \"unique_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171c1b405d88408",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new_part.show(n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79379063057bfee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:  \n",
    "# medical condition to be  chosen \n",
    "# tests to be chosen\n",
    "# admission date to be checked again dob,  \n",
    "# TODO filter on is pediatric, geriatric and is_female to be done here and same people with dob? needs  to be considered \n",
    "# drop stay_name and unique_id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb59205ef488cec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = join_with_condition_prob_df.select([col(c).cast(StringType()).alias(c) for c in join_with_condition_prob_df.columns])\n",
    "# df.show()\n",
    "# df.repartition(10).write.csv('./temp_data/join_with_condition_prob_df/renamed.csv', mode = 'overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0099b41d9dc86",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986e68bfc10315d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d89b5173e46238",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
