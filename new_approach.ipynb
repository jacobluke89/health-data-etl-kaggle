{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f364a0ecf68e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from src.data_generator.csv_data_processor import CSVDataProcessor\n",
    "from src.utils.statistcs.statistical_functions import create_distributed_age_df\n",
    "from src.utils.read_write import read_postgres_table\n",
    "from spark_instance import spark\n",
    "from src.utils.statistcs.data_visualisations import plot_age_distribution_with_sd, plot_kernel_density_age_distribution\n",
    "\n",
    "from pyspark.sql.functions import col, lower, array, concat_ws, date_format, sum, lit\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_age = 40.2\n",
    "\n",
    "csv_age_file_file = CSVDataProcessor(spark, \"data/uk_age_population.csv\")\n",
    "\n",
    "csv_age_uk_df = csv_age_file_file.runner()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48c373a8586c425e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_age_uk_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3be2c9b63747804"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_age_distribution_with_sd(csv_age_uk_df, average_age)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b067d44ccbd2c1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_kernel_density_age_distribution(csv_age_uk_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5e2fc658fbefcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df = create_distributed_age_df(spark,  \"data/uk_age_population.csv\", 5000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbc8ab960c79ce19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41080574f0f2cf54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_aggregated = sampled_df.groupby(col(\"Age\")).count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6632f60001d9078"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_aggregated.orderBy(\"Age\").show(n=200)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b8df9b39afdebc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_age_distribution_with_sd(sampled_df_aggregated)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71e065f8031e4003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def create_unique_id(non_unique_id_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans a name column by removing spaces and periods, and then concatenates it with a date column to form a unique ID.\n",
    "\n",
    "    Args:\n",
    "        non_unique_id_df (DataFrame): The input DataFrame that contains the name and date of birth columns.\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with an added column that combines cleaned names with dates of birth into a unique ID.\n",
    "    \"\"\"\n",
    "    # Clean the 'name' column by removing periods and replacing spaces with underscores\n",
    "    cleaned_df = non_unique_id_df.withColumn(\n",
    "        \"clean_name\",\n",
    "        regexp_replace(regexp_replace(col(\"name\"), \"\\\\.\", \"\"), \"\\\\s\", \"_\")\n",
    "    )\n",
    "\n",
    "    return cleaned_df.withColumn(\n",
    "        \"unique_id\",\n",
    "        concat_ws(\"_\", col(\"clean_name\"), col(\"DOB\"))\n",
    "    ).drop(\"clean_name\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4566a4fa77c2644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "is_female = lower(col('gender')) == 'female'\n",
    "is_pediatric = col(\"Age\") < 18\n",
    "is_geriatric = (col(\"Age\") >= 65)\n",
    "\n",
    "sampled_df_with_name = (sampled_df.withColumn(\"is_female\", is_female)\n",
    "                                  .withColumn(\"is_pediatric\", is_pediatric)\n",
    "                                  .withColumn(\"is_geriatric\", is_geriatric))\n",
    "sampled_df_with_unique_id = create_unique_id(sampled_df_with_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f87bda80e7f65562"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_with_unique_id.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a41e27033b8ff8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampled_df_with_unique_id.createOrReplaceGlobalTempView(\"sampled_df_with_unique_id_gt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4703b95b3fdbf13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# will be scrapping this shortly \n",
    "csv_reader = CSVDataProcessor(spark, \"data/healthcare_dataset.csv\")\n",
    "\n",
    "# Read the CSV file\n",
    "raw_df = csv_reader.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, floor, datediff, current_date\n",
    "\n",
    "min_age_days = 1 * 365  # Minimum age in days (18 years)\n",
    "max_age_days = 90 * 365  # Maximum age in days (90 years)\n",
    "\n",
    "raw_df = (raw_df.withColumn(\"DOB\", expr(f\"date_sub(current_date(), CAST(round(rand() * ({max_age_days} - {min_age_days}) + {min_age_days}) AS INT))\"))\n",
    "        .withColumn(\"Age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25)))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a869847c23198ca2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.read_write import write_postgres_table\n",
    "\n",
    "write_postgres_table(raw_df, \"dob_age_raw_data\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d912825134cc4196"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3e561ca62f93",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# above is commented out as it's saved in the database:\n",
    "df = read_postgres_table(\"dob_age_raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42a8509dc84e270"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463e373907bf09e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.constants.admission_types_dataset import admission_mapping, AdmissionTypes\n",
    "flattened = [\n",
    "    (\n",
    "        top_level.name,\n",
    "        sub_level_key.name,\n",
    "        list(sub_level_info.get(\"stay_types\")),\n",
    "        sub_level_info.get(\"tests\"),  \n",
    "        random.choice(sub_level_info.get(\"doctors\"))\n",
    "    )\n",
    "    for top_level, sub_level_dict in admission_mapping.items()\n",
    "    for sub_level_key, sub_level_info in sub_level_dict.items()\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa93b831b3aed7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76324a29d381e7c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.constants.condition_probabilities import condition_age_probability_dict\n",
    "\n",
    "flattened_condition_probabilities = [\n",
    "    (\n",
    "        sub_admission,\n",
    "        condition,\n",
    "        gender if gender in ['male', 'female'] else None,\n",
    "        float(age_range[0]),\n",
    "        float(age_range[1]) if len(age_range) > 1 else float('inf'),\n",
    "        float(probability) \n",
    "    )\n",
    "    for sub_admission, conditions in condition_age_probability_dict.items()\n",
    "    for condition, genders_or_age_prob_list in conditions.items()\n",
    "    for gender, age_prob_list in (genders_or_age_prob_list.items() if isinstance(genders_or_age_prob_list, dict) else [(None, genders_or_age_prob_list)])\n",
    "    for age_range, probability in age_prob_list\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flattened_condition_probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d8352a033cdbc4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa988bdaed2265",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "columns = [\"condition_admission_type\", \"condition\", \"gender\", \"age_min\", \"age_max\", \"probability\"]\n",
    "schema = StructType([\n",
    "    StructField(\"condition_admission_type\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True),\n",
    "    StructField(\"condition_gender\", StringType(), True),  # Assuming gender can be specific probability for gender 'male', 'female', or null\n",
    "    StructField(\"age_min\", FloatType(), False),\n",
    "    StructField(\"age_max\", FloatType(), False),\n",
    "    StructField(\"probability\", FloatType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "condition_prob_df = spark.createDataFrame(data=flattened_condition_probabilities, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a88bdeea4c696e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_prob_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc302cbd5cd49e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1 \n",
    "    From the flattened data list we wish to create a DataFrame. This contains all the possible combinations for the given \n",
    "    top level admissions, sub level admissions, stay types and list of tests available from the admission_mapping, stay_type and admission_tests lists or dictionaries in admission_types_test_dataset.py\n",
    "\"\"\"\n",
    "\n",
    "mapping_df = spark.createDataFrame(flattened, [\"top_level_admission\", \"sub_level_admission\", \"stay_types\", \"possible_tests\", \"doctor\"])\n",
    "\n",
    "joined_tbl = mapping_df.join(condition_prob_df, on=[mapping_df.sub_level_admission == condition_prob_df.condition_admission_type], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gt_df = spark.sql(\"SELECT * FROM global_temp.sampled_df_with_unique_id_gt\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8e1ecc6cc582799"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 3  \n",
    "Create Enum class df and join it on to main driver df. \n",
    "Then create conditions \n",
    "\"\"\"\n",
    "\n",
    "from src.data_generator.conditions_creator import ConditionsCreator\n",
    "from src.utils.thread_operations import runner\n",
    "from src.constants.type_constants import DepartmentTypes\n",
    "\n",
    "enum_values = [e.name for e in DepartmentTypes]\n",
    "\n",
    "enum_df = spark.createDataFrame(enum_values, StringType()).toDF(\"admission_type\")\n",
    "\n",
    "selected_conditions_df = runner(spark, ConditionsCreator, gt_df, joined_tbl, enum_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "632611cc8f270e95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.util_funcs import get_row_count\n",
    "\n",
    "get_row_count(selected_conditions_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c3e16d26fc1e5f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_conditions_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c737cfa61730f556"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79379063057bfee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:  \n",
    "# tests to be chosen\n",
    "# admission date to be chosen \n",
    "# hospital name \n",
    "# patient postcode \n",
    "# ethnicity distribution  \n",
    "# _________________\n",
    "# Long term \n",
    "# investigation which disease affects specific ethnicities\n",
    "# blood type udf would need to consider ethnicity also eventually\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0099b41d9dc86",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
